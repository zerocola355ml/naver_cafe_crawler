import sys
import io

# Windows ì½˜ì†” ?ï¿½ì½”???ï¿½ì • (?ï¿½ï¿½? ï¿½??ï¿½ìˆ˜ë¬¸ì ì¶œë ¥???ï¿½í•´)
# line_buffering=Trueï¿½??ï¿½ì •?ï¿½ì—¬ ?ï¿½ì‹œï¿½?ì¶œë ¥ ê°€??
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', line_buffering=True)
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', line_buffering=True)

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException
import os
import time
import re
import sqlite3
from datetime import datetime, timedelta

# ===================== Logger ?ï¿½ë˜??=====================
class Logger:
    """ë¡œê¹…ë ˆë²¨ ê´€ë¦¬"""
    VERBOSE = 2  # ëª¨ë“  ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
    INFO = 1     # ì‚¬ìš©ì ì •ë³´ë§Œ ì¶œë ¥
    QUIET = 0    # ìµœì†Œ ì •ë³´ë§Œ ì¶œë ¥ (ì—ëŸ¬/ê²½ê³ )
    
    level = INFO  # ê¸°ë³¸ ë ˆë²¨
    
    @staticmethod
    def set_level(level):
        """ë¡œê¹… ë ˆë²¨ ì„¤ì •"""
        Logger.level = level
    
    @staticmethod
    def debug(msg):
        """ë””ë²„ê·¸ ë©”ì‹œì§€ (VERBOSE ë ˆë²¨ì—ì„œë§Œ ì¶œë ¥)"""
        if Logger.level >= Logger.VERBOSE:
            print(f"[DEBUG] {msg}")
    
    @staticmethod
    def info(msg):
        """ì¼ë°˜ ì •ë³´ ë©”ì‹œì§€ (INFO ë ˆë²¨ ì´ìƒì—ì„œ ì¶œë ¥)"""
        if Logger.level >= Logger.INFO:
            print(msg)
    
    @staticmethod
    def success(msg):
        """ì„±ê³µ ë©”ì‹œì§€ (INFO ë ˆë²¨ ì´ìƒì—ì„œ ì¶œë ¥)"""
        if Logger.level >= Logger.INFO:
            print(f"âœ“ {msg}")
    
    @staticmethod
    def warning(msg):
        """ê²½ê³  ë©”ì‹œì§€ (í•­ìƒ ì¶œë ¥)"""
        print(f"âš ï¸ {msg}")
    
    @staticmethod
    def error(msg):
        """ì—ëŸ¬ ë©”ì‹œì§€ (í•­ìƒ ì¶œë ¥)"""
        print(f"âŒ {msg}")
    
    @staticmethod
    def separator(char="=", length=60):
        """êµ¬ë¶„ì„  (INFO ë ˆë²¨ ì´ìƒì—ì„œ ì¶œë ¥)"""
        if Logger.level >= Logger.INFO:
            print(char * length)

# ===================== ì„¤ì • í´ë˜ìŠ¤ =====================
class Config:
    """ìŠ¤í¬ë˜í•‘ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    # URL ì„¤ì •
    DEFAULT_URL = "https://cafe.naver.com/f-e/cafes/10094499/menus/599?viewType=L&page=1"
    
    # ë¡œê¹… ì„¤ì •
    LOG_LEVEL = Logger.INFO  # VERBOSE(ìì„¸), INFO(ë³´í†µ), QUIET(ìµœì†Œ)
    
    # ê²Œì‹œê¸€ í•„í„° ì„¤ì •
    SKIP_NOTICE = True      # ê³µì§€ ì™¸
    SKIP_RECOMMEND = True   # ì¶”ì²œê¸€ ì™¸
    
    # ë¸Œë¼ìš°ì € ì„¤ì •
    USE_PROFILE = False     # Chrome í”„ë¡œí•„ ì‚¬ìš©
    CHROME_PROFILE_PATH = "C:\\Users\\tlsgj\\AppData\\Local\\Google\\Chrome\\User Data"
    PROFILE_DIRECTORY = "Default"

    # í˜ì´ì§€ ë¡œë”© ì„¤ì •
    PAGE_LOAD_WAIT = 15     # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    ELEMENT_WAIT = 20       # ìš”ì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    SELECTOR_WAIT = 5       # ì„ íƒì ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    
    # ì¶œë ¥ íŒŒì¼ ì„¤ì •
    OUTPUT_FILE = "scraped_articles.txt"
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    DB_FILE = "naver_cafe_articles.db"  # SQLite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼
    RETENTION_DAYS = 15                  # ë³´ê´€ ê¸°ê°„ (ì¼)
    
    # ì¸ê¸°ê¸€ í•„í„° ê¸°ì¤€ (AND ì¡°ê±´: ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•¨)
    HOT_ARTICLE_MIN_LIKE = 10       # ìµœì†Œ ì¢‹ì•„ìš” ìˆ˜
    HOT_ARTICLE_MIN_READ = 100      # ìµœì†Œ ì¡°íšŒìˆ˜
    HOT_ARTICLE_MIN_COMMENT = 5     # ìµœì†Œ ëŒ“ê¸€ ìˆ˜
    
    # í‚¤ì›Œë“œ í•„í„° (ì¸ê¸°ê¸€ ì¤‘ í‚¤ì›Œë“œ í¬í•¨ ê²Œì‹œê¸€ ë³„ë„ ì¶”ì )
    KEYWORDS = ['ê¸°ì €ê·€', 'ìœ ì‚°ê· ', 'ë°”ì´ì˜¤ê°€ì´ì•„', 'ë¬¼í‹°ìŠˆ']  # ê´€ì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    
    # í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì„¤ì •
    TELEGRAM_ENABLED = True  # í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‚¬ìš© ì—¬ë¶€
    
    # í…”ë ˆê·¸ë¨ í† í° (config_secret.pyì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸° - GitHubì— ì•ˆì „)
    try:
        from config_secret import TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID
    except ImportError:
        TELEGRAM_BOT_TOKEN = ""  # config_secret.pyê°€ ì—†ìœ¼ë©´ ë¹ˆ ê°’
        TELEGRAM_CHAT_ID = ""
    
    # ìŠ¤í¬ë˜í•‘ ë²”ìœ„ ì„¤ì •
    SCRAPE_DAYS = 7         # ìµœê·¼ ë©°ì¹  ë™ì•ˆì˜ ê²Œì‹œê¸€ë§Œ ìˆ˜ì§‘ (ì˜¤ëŠ˜ë¶€í„° Nì¼ ì „ê¹Œì§€)
    MAX_PAGES = 50          # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
    
    # CSS Selector ì„¤ì •
    SELECTORS = {
        # ê²Œì‹œê¸€ í–‰(tr) ì°¾ê¸°
        'article_rows': [
            "table.article-table > tbody:nth-of-type(3) tr",  # ì¼ë°˜ê¸€(3ë²ˆì§¸ tbody)
            "table.article-table tbody:nth-of-type(3) tr",
            "table.article-table tbody:nth-of-type(n+2) tr",  # ì¶”ì²œê¸€ + ì¼ë°˜ê¸€
            "table.article-table tr",  # ì „ì²´
        ],
        
        # ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
        'article_id': "td.td_normal.type_articleNumber",
        'title': "a.article",
        'comment': "a.cmt",
        'date': "td.td_normal.type_date",
        'read_count': "td.td_normal.type_readCount",
        'like_count': "td.td_normal.type_likeCount",
    }

# ===================== ë°ì´í„°ë² ì´ìŠ¤ í•¨ìˆ˜ =====================

def init_database():
    """
    SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    í…Œì´ë¸”ì´ ì¡´ì¬í•˜ë©´ ìƒì„±í•˜ê³ , ì—°ê²°ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        sqlite3.Connection: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    """
    conn = sqlite3.connect(Config.DB_FILE)
    cursor = conn.cursor()
    
    # articles í…Œì´ë¸” ìƒì„±
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS articles (
            article_id TEXT PRIMARY KEY,
            title TEXT NOT NULL,
            comment_count INTEGER DEFAULT 0,
            date TEXT,
            read_count INTEGER DEFAULT 0,
            like_count INTEGER DEFAULT 0,
            url TEXT,
            first_scraped TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # hot_articles í…Œì´ë¸” ìƒì„± (ì¸ê¸°ê¸€ ì¶”ì  ë° ì•Œë¦¼ìš©)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS hot_articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            article_id TEXT NOT NULL,
            title TEXT,
            comment_count INTEGER,
            read_count INTEGER,
            like_count INTEGER,
            url TEXT,
            date TEXT,
            detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            notification_sent BOOLEAN DEFAULT 0,
            notification_sent_at TIMESTAMP,
            notification_method TEXT,
            FOREIGN KEY (article_id) REFERENCES articles(article_id)
        )
    ''')
    
    # hot_articlesì— ì¸ë±ìŠ¤ ì¶”ê°€ (ë¹ ë¥¸ ì¡°íšŒ)
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_hot_articles_article_id 
        ON hot_articles(article_id)
    ''')
    
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_hot_articles_notification 
        ON hot_articles(notification_sent)
    ''')
    
    # keyword_articles í…Œì´ë¸” ìƒì„± (ì¸ê¸°ê¸€ + í‚¤ì›Œë“œ í¬í•¨)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS keyword_articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            article_id TEXT NOT NULL,
            title TEXT,
            comment_count INTEGER,
            read_count INTEGER,
            like_count INTEGER,
            url TEXT,
            date TEXT,
            matched_keywords TEXT,
            detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            notification_sent BOOLEAN DEFAULT 0,
            notification_sent_at TIMESTAMP,
            notification_method TEXT,
            FOREIGN KEY (article_id) REFERENCES articles(article_id)
        )
    ''')
    
    # keyword_articlesì— ì¸ë±ìŠ¤ ì¶”ê°€
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_keyword_articles_article_id 
        ON keyword_articles(article_id)
    ''')
    
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_keyword_articles_notification 
        ON keyword_articles(notification_sent)
    ''')
    
    conn.commit()
    Logger.debug(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ: {Config.DB_FILE}")
    return conn


def cleanup_old_data(conn):
    """
    ë³´ê´€ ê¸°ê°„ ì´ì „ì˜ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        conn: SQLite ì—°ê²°
    
    Returns:
        int: ì‚­ì œëœ ë°ì´í„°ì˜ ê°œìˆ˜
    """
    cursor = conn.cursor()
    cutoff_date = datetime.now() - timedelta(days=Config.RETENTION_DAYS)
    
    cursor.execute('''
        DELETE FROM articles 
        WHERE last_updated < ?
    ''', (cutoff_date.strftime('%Y-%m-%d %H:%M:%S'),))
    
    deleted_count = cursor.rowcount
    conn.commit()
    
    if deleted_count > 0:
        Logger.info(f"{deleted_count}ê°œì˜ ì˜¤ë˜ëœ ê²Œì‹œê¸€ ì‚­ì œ ({Config.RETENTION_DAYS}ì¼ ì´ìƒ)")
    
    return deleted_count


def save_or_update_article(conn, article):
    """
    ê²Œì‹œê¸€ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    
    Args:
        conn: SQLite ì—°ê²°
        article: ê²Œì‹œê¸€ ì •ë³´ ì‚¬ì „
    
    Returns:
        str: 'inserted', 'updated', ë˜ëŠ” 'error'
    """
    cursor = conn.cursor()
    
    try:
        # ê¸°ì¡´ ê²Œì‹œê¸€ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        cursor.execute('SELECT article_id FROM articles WHERE article_id = ?', (article['article_id'],))
        exists = cursor.fetchone()
        
        now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        if exists:
            # ë°ì´í„° ì—…ë°ì´íŠ¸
            cursor.execute('''
                UPDATE articles 
                SET title = ?, comment_count = ?, date = ?, 
                    read_count = ?, like_count = ?, url = ?, last_updated = ?
                WHERE article_id = ?
            ''', (
                article['title'], article['comment_count'], article['date'],
                article['read_count'], article['like_count'], article['url'], 
                now, article['article_id']
            ))
            conn.commit()
            return 'updated'
        else:
            # ìƒˆ ê²Œì‹œê¸€ ì‚½ì…
            cursor.execute('''
                INSERT INTO articles 
                (article_id, title, comment_count, date, read_count, like_count, url, first_scraped, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article['article_id'], article['title'], article['comment_count'],
                article['date'], article['read_count'], article['like_count'],
                article['url'], now, now
            ))
            conn.commit()
            return 'inserted'
    except Exception as e:
        Logger.debug(f"DB ì˜¤ë¥˜ (ID: {article.get('article_id')}): {e}")
        return 'error'


def get_article_stats(conn):
    """
    ?ï¿½ì´?ï¿½ë² ?ï¿½ìŠ¤ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        conn: SQLite ì—°ê²°
    
    Returns:
        dict: í†µê³„ ì •ë³´
    """
    cursor = conn.cursor()
    
    # ì´ ê²Œì‹œê¸€ ìˆ˜
    cursor.execute('SELECT COUNT(*) FROM articles')
    total_count = cursor.fetchone()[0]
    
    # ì˜¤ëŠ˜ ì—…ë°ì´íŠ¸ëœ ê²Œì‹œê¸€ ìˆ˜
    today = datetime.now().strftime('%Y-%m-%d')
    cursor.execute('SELECT COUNT(*) FROM articles WHERE DATE(last_updated) = ?', (today,))
    today_updated = cursor.fetchone()[0]
    
    # ê°€???ï¿½ë˜??ê²Œì‹œê¸€
    cursor.execute('SELECT MIN(first_scraped) FROM articles')
    oldest = cursor.fetchone()[0]
    
    return {
        'total_count': total_count,
        'today_updated': today_updated,
        'oldest_date': oldest
    }


def is_hot_article(article):
    """
    ê²Œì‹œê¸€ì´ ì¸ê¸°ê¸€ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤ (AND ì¡°ê±´).
    
    Args:
        article: ê²Œì‹œê¸€ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        bool: ì¸ê¸°ê¸€ì´ë©´ True
    """
    return (
        article.get('like_count', 0) >= Config.HOT_ARTICLE_MIN_LIKE and
        article.get('read_count', 0) >= Config.HOT_ARTICLE_MIN_READ and
        article.get('comment_count', 0) >= Config.HOT_ARTICLE_MIN_COMMENT
    )


def save_hot_article(conn, article):
    """
    ì¸ê¸°ê¸€ì„ hot_articles í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
    ì´ë¯¸ ì €ì¥ëœ ê²Œì‹œê¸€ì€ ì¤‘ë³µ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    
    Args:
        conn: SQLite ì—°ê²°
        article: ê²Œì‹œê¸€ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        str: 'inserted', 'exists', ë˜ëŠ” 'error'
    """
    cursor = conn.cursor()
    
    try:
        # ì´ë¯¸ hot_articlesì— ìˆëŠ”ì§€ í™•ì¸
        cursor.execute('''
            SELECT id FROM hot_articles 
            WHERE article_id = ?
        ''', (article['article_id'],))
        
        if cursor.fetchone():
            Logger.debug(f"ì¸ê¸°ê¸€ ì´ë¯¸ ì €ì¥ë¨: {article['article_id']}")
            return 'exists'
        
        # ìƒˆë¡œ ì¶”ê°€
        now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        cursor.execute('''
            INSERT INTO hot_articles 
            (article_id, title, comment_count, read_count, like_count, url, date, detected_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            article['article_id'],
            article['title'],
            article['comment_count'],
            article['read_count'],
            article['like_count'],
            article['url'],
            article['date'],
            now
        ))
        
        conn.commit()
        Logger.info(f"ğŸ”¥ ì¸ê¸°ê¸€ ë°œê²¬: {article['title'][:30]}... (ì¢‹ì•„ìš”:{article['like_count']}, ì¡°íšŒ:{article['read_count']}, ëŒ“ê¸€:{article['comment_count']})")
        return 'inserted'
        
    except Exception as e:
        Logger.debug(f"ì¸ê¸°ê¸€ ì €ì¥ ì˜¤ë¥˜ (ID: {article.get('article_id')}): {e}")
        return 'error'


def get_hot_article_stats(conn):
    """
    ì¸ê¸°ê¸€ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        conn: SQLite ì—°ê²°
    
    Returns:
        dict: ì¸ê¸°ê¸€ í†µê³„
    """
    cursor = conn.cursor()
    
    # ì´ ì¸ê¸°ê¸€ ìˆ˜
    cursor.execute('SELECT COUNT(*) FROM hot_articles')
    total_hot = cursor.fetchone()[0]
    
    # ì˜¤ëŠ˜ ë°œê²¬ëœ ì¸ê¸°ê¸€ ìˆ˜
    today = datetime.now().strftime('%Y-%m-%d')
    cursor.execute('SELECT COUNT(*) FROM hot_articles WHERE DATE(detected_at) = ?', (today,))
    today_hot = cursor.fetchone()[0]
    
    # ì•Œë¦¼ ë¯¸ë°œì†¡ ì¸ê¸°ê¸€ ìˆ˜
    cursor.execute('SELECT COUNT(*) FROM hot_articles WHERE notification_sent = 0')
    pending_notification = cursor.fetchone()[0]
    
    return {
        'total_hot': total_hot,
        'today_hot': today_hot,
        'pending_notification': pending_notification
    }


def check_keywords(article, keywords):
    """
    ê²Œì‹œê¸€ ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        article: ê²Œì‹œê¸€ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        list: ë§¤ì¹­ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
    """
    title = article.get('title', '').lower()
    matched = []
    
    for keyword in keywords:
        if keyword.lower() in title:
            matched.append(keyword)
    
    return matched


def save_keyword_article(conn, article, matched_keywords):
    """
    í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ì¸ê¸°ê¸€ì„ keyword_articles í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        conn: SQLite ì—°ê²°
        article: ê²Œì‹œê¸€ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        matched_keywords: ë§¤ì¹­ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        str: 'inserted', 'exists', ë˜ëŠ” 'error'
    """
    cursor = conn.cursor()
    
    try:
        # ì´ë¯¸ keyword_articlesì— ìˆëŠ”ì§€ í™•ì¸
        cursor.execute('''
            SELECT id FROM keyword_articles 
            WHERE article_id = ?
        ''', (article['article_id'],))
        
        if cursor.fetchone():
            Logger.debug(f"í‚¤ì›Œë“œ ì¸ê¸°ê¸€ ì´ë¯¸ ì €ì¥ë¨: {article['article_id']}")
            return 'exists'
        
        # ìƒˆë¡œ ì¶”ê°€
        now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        keywords_str = ', '.join(matched_keywords)
        
        cursor.execute('''
            INSERT INTO keyword_articles 
            (article_id, title, comment_count, read_count, like_count, url, date, 
             matched_keywords, detected_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            article['article_id'],
            article['title'],
            article['comment_count'],
            article['read_count'],
            article['like_count'],
            article['url'],
            article['date'],
            keywords_str,
            now
        ))
        
        conn.commit()
        Logger.info(f"â­ í‚¤ì›Œë“œ ì¸ê¸°ê¸€ ë°œê²¬: {article['title'][:30]}... [í‚¤ì›Œë“œ: {keywords_str}]")
        return 'inserted'
        
    except Exception as e:
        Logger.debug(f"í‚¤ì›Œë“œ ì¸ê¸°ê¸€ ì €ì¥ ì˜¤ë¥˜ (ID: {article.get('article_id')}): {e}")
        return 'error'


def get_keyword_article_stats(conn):
    """
    í‚¤ì›Œë“œ ì¸ê¸°ê¸€ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        conn: SQLite ì—°ê²°
    
    Returns:
        dict: í‚¤ì›Œë“œ ì¸ê¸°ê¸€ í†µê³„
    """
    cursor = conn.cursor()
    
    # ì´ í‚¤ì›Œë“œ ì¸ê¸°ê¸€ ìˆ˜
    cursor.execute('SELECT COUNT(*) FROM keyword_articles')
    total_keyword = cursor.fetchone()[0]
    
    # ì˜¤ëŠ˜ ë°œê²¬ëœ í‚¤ì›Œë“œ ì¸ê¸°ê¸€ ìˆ˜
    today = datetime.now().strftime('%Y-%m-%d')
    cursor.execute('SELECT COUNT(*) FROM keyword_articles WHERE DATE(detected_at) = ?', (today,))
    today_keyword = cursor.fetchone()[0]
    
    # ì•Œë¦¼ ë¯¸ë°œì†¡ í‚¤ì›Œë“œ ì¸ê¸°ê¸€ ìˆ˜
    cursor.execute('SELECT COUNT(*) FROM keyword_articles WHERE notification_sent = 0')
    pending_notification = cursor.fetchone()[0]
    
    return {
        'total_keyword': total_keyword,
        'today_keyword': today_keyword,
        'pending_notification': pending_notification
    }


# ===================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =====================

def parse_article_date(date_str):
    """
    ê²Œì‹œê¸€ ìƒì„±ì¼ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    
    Args:
        date_str: ë‚ ì§œ ë¬¸ìì—´("09:05" ë˜ëŠ” "2025.10.26.")
    
    Returns:
        datetime: íŒŒì‹±ëœ ë‚ ì§œ ë˜ëŠ” None
    """
    try:
        date_str = date_str.strip()
        
        # ì‹œê°„ í˜•ì‹ (ì˜¤ëŠ˜ ê²Œì‹œê¸€): "09:05"
        if ':' in date_str and '.' not in date_str:
            today = datetime.now()
            time_parts = date_str.split(':')
            hour = int(time_parts[0])
            minute = int(time_parts[1])
            return today.replace(hour=hour, minute=minute, second=0, microsecond=0)
        
        # ë‚ ì§œ í˜•ì‹: "2025.10.26." ë˜ëŠ” "2025.10.26"
        elif '.' in date_str:
            date_str = date_str.rstrip('.')
            parts = date_str.split('.')
            if len(parts) >= 3:
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                return datetime(year, month, day)
        
        return None
    except Exception as e:
        Logger.debug(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_str} - {e}")
        return None


def is_article_too_old(date_str, days_limit):
    """
    ê²Œì‹œê¸€ì´ ì˜¤ë˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        date_str: ë‚ ì§œ ë¬¸ìì—´
        days_limit: ë©°ì¹  ì´ì „ê¹Œì§€ ìˆ˜ì§‘í• ì§€
    
    Returns:
        bool: True(ì¤‘ë‹¨í•´ì•¼ í•¨)
    """
    article_date = parse_article_date(date_str)
    if not article_date:
        return False
    
    cutoff_date = datetime.now() - timedelta(days=days_limit)
    return article_date < cutoff_date


def generate_page_url(base_url, page_number):
    """
    í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ URLì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        base_url: ê¸°ë³¸ URL
        page_number: í˜ì´ì§€ ë²ˆí˜¸
    
    Returns:
        str: í˜ì´ì§€ URL
    """
    # URLì—ì„œ page= ë¶€ë¶„ì„ ì°¾ì•„ êµì²´
    if 'page=' in base_url:
        # ê¸°ì¡´ page= íŒŒë¼ë¯¸í„° êµì²´
        return re.sub(r'page=\d+', f'page={page_number}', base_url)
    elif '?' in base_url:
        # page íŒŒë¼ë¯¸í„° ì¶”ê°€
        return f"{base_url}&page={page_number}"
    else:
        return f"{base_url}?page={page_number}"


def setup_chrome_driver():
    """
    Chrome WebDriverë¥¼ ì„¤ì •í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        webdriver.Chrome: ì„¤ì •ëœ Chrome WebDriver
    """
    options = Options()

    # í”„ë¡œí•„ ì‚¬ìš© ì„¤ì •
    if Config.USE_PROFILE:
        Logger.debug("í”„ë¡œí•„ ëª¨ë“œ ì‹¤í–‰")
        profile_path = os.path.join(Config.CHROME_PROFILE_PATH, Config.PROFILE_DIRECTORY)
        if not os.path.isdir(profile_path):
            Logger.error("í”„ë¡œí•„ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. USE_PROFILEë¥¼ Falseë¡œ ì„¤ì •í•˜ì„¸ìš”.")
            return None
        options.add_argument(f"user-data-dir={Config.CHROME_PROFILE_PATH}")
        options.add_argument(f"profile-directory={Config.PROFILE_DIRECTORY}")
    
    # Chrome ì˜µì…˜ (ìë™í™” ì œì–´ ë¹„í™œì„±í™”)
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument("--start-maximized")
    
    try:
        Logger.info("\në¸Œë¼ìš°ì € ì‘ì—… ì‹œì‘...")
        driver = webdriver.Chrome(options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        Logger.success("ë¸Œë¼ìš°ì € ì‘ì—… ì™„ë£Œ")
        return driver
    except Exception as e:
        Logger.error("WebDriver ì‹¤í–‰ ì‹¤íŒ¨.")
        Logger.info(f"ì˜¤ë¥˜: {e}")
        Logger.info("\n?ï¿½ê²° ë°©ë²•:")
        Logger.info("1. Chrome ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
        Logger.info("2. ChromeDriverê°€ ìë™ìœ¼ë¡œ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
        return None


def extract_article_data(row):
    """
    tr)ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        row: Selenium WebElement (tr ê·¸ë˜)
    
    Returns:
        dict: ê²Œì‹œê¸€ ì •ë³´ ì‚¬ì „ ë˜ëŠ” None (ì¶”ì¶œ ì‹¤íŒ¨)
    """
    article_data = {
        'article_id': '',
        'title': '',
        'comment_count': 0,
        'date': '',
        'read_count': 0,
        'like_count': 0,
        'url': ''
    }
    
    try:
        # ê²Œì‹œê¸€ ID ì¶”ì¶œ (tdì—ì„œ ì§ì ‘)
        try:
            article_id_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['article_id'])
            article_data['article_id'] = article_id_elem.text.strip()
        except:
            pass
        
        # ì œëª© ì¶”ì¶œ (í•„ìˆ˜)
        try:
            title_link = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['title'])
            article_data['title'] = title_link.text.strip()
            article_data['url'] = title_link.get_attribute('href')
            
            # IDê°€ ì—†ìœ¼ë©´ URLì—ì„œ ì¶”ì¶œ
            if not article_data['article_id']:
                url = article_data['url']
                if 'articleid=' in url.lower():
                    article_data['article_id'] = url.split('articleid=')[1].split('&')[0]
                elif '/articles/' in url:
                    article_data['article_id'] = url.split('/articles/')[1].split('?')[0]
        except Exception as e:
            Logger.debug(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
        
        # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
        try:
            comment_link = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['comment'])
            comment_text = comment_link.text.strip()
            numbers = re.findall(r'\d+', comment_text)
            if numbers:
                article_data['comment_count'] = int(numbers[0])
        except:
            article_data['comment_count'] = 0
        
        # ìƒì„±ì¼ ì¶”ì¶œ
        try:
            date_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['date'])
            article_data['date'] = date_elem.text.strip()
        except:
            article_data['date'] = ''
        
        # ì¡°íšŒìˆ˜ ì¶”ì¶œ
        try:
            read_count_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['read_count'])
            read_count_text = read_count_elem.text.strip()
            numbers = re.findall(r'\d+', read_count_text.replace(',', ''))
            if numbers:
                article_data['read_count'] = int(numbers[0])
        except:
            article_data['read_count'] = 0
        
        # ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ
        try:
            like_count_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['like_count'])
            like_count_text = like_count_elem.text.strip()
            numbers = re.findall(r'\d+', like_count_text.replace(',', ''))
            if numbers:
                article_data['like_count'] = int(numbers[0])
        except:
            article_data['like_count'] = 0
        
        return article_data
        
    except Exception as e:
        Logger.debug(f"ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None


def should_skip_article(row, driver, skip_notice=True, skip_recommend=True):
    """
    ê²Œì‹œê¸€ ê³µì§€ í•­ëª© ì™¸ëŠ” ì¶”ì²œê¸€ ì™¸ì¸ ì—¬ë¶€ì— ë”°ë¼ ê²°ì •í•©ë‹ˆë‹¤.
    
    Args:
        row: Selenium WebElement (tr ê·¸ë˜)
        driver: Selenium WebDriver
        skip_notice: ê³µì§€ í•­ëª© ì™¸ ì—¬ë¶€
        skip_recommend: ì¶”ì²œê¸€ ì™¸ ì—¬ë¶€
    
    Returns:
        tuple: (skip: bool, reason: str) - ê±´ë„ˆë›°ì–´ì•¼ í•˜ëŠ” ì´ìœ 
    """
    try:
        # ì´ì— í•´ë‹¹í•˜ëŠ” tbody ì°¾ê¸°
        tbody = driver.execute_script("""
            let elem = arguments[0];
            while (elem && elem.tagName !== 'TBODY') {
                elem = elem.parentElement;
            }
            return elem;
        """, row)
        
        if tbody:
            # tbody ë¶€ë¶„ì— table ì°¾ê¸°
            table = driver.execute_script("return arguments[0].parentElement;", tbody)
            if table and 'article-table' in (table.get_attribute('class') or ''):
                # ê°™ì€ tableì— ëª¨ë“  tbody ê°€ì ¸ì˜¤ê¸°
                all_tbodies = driver.execute_script(
                    "return arguments[0].querySelectorAll('tbody');", table
                )
                # í˜„ì¬ tbody ì¸ë±ìŠ¤ ì°¾ê¸°
                tbody_index = -1
                for idx, tb in enumerate(all_tbodies):
                    if tb == tbody:
                        tbody_index = idx
                        break
                
                # tbody ê°œìˆ˜ì— ë”°ë¼ êµ¬ë¶„
                # - 3ê°œ: 0=ê³µì§€, 1=ì¶”ì²œ, 2=ì¼ë°˜
                # - 1ê°œ: ì „ë¶€ ì¼ë°˜ê¸€ (í˜ì´ì§€ 2 ì´í›„)
                Logger.debug(f"tbody ì¸ë±ìŠ¤: {tbody_index} (ì´ {len(all_tbodies)}ê°œ tbody)")
                
                # tbodyê°€ 3ê°œ ì´ìƒì¼ ë•Œë§Œ ì¸ë±ìŠ¤ë¡œ êµ¬ë¶„
                if len(all_tbodies) >= 3:
                    if tbody_index == 0 and skip_notice:
                        Logger.debug(f"ê³µì§€ì‚¬í•­ìœ¼ë¡œ ìŠ¤í‚µ")
                        return (True, 'notice')
                    elif tbody_index == 1 and skip_recommend:
                        Logger.debug(f"ì¶”ì²œê¸€ë¡œ ìŠ¤í‚µ")
                        return (True, 'recommend')
                    elif tbody_index >= 2:
                        Logger.debug(f"ì¼ë°˜ê¸€ë¡œ ì¸ì‹ (tbody_index={tbody_index})")
                        return (False, None)
                else:
                    # tbodyê°€ 1-2ê°œë©´ ì „ë¶€ ì¼ë°˜ê¸€
                    Logger.debug(f"ì¼ë°˜ê¸€ë¡œ ì¸ì‹ (tbody ê°œìˆ˜: {len(all_tbodies)})")
                    return (False, None)
    except Exception as e:
        Logger.debug(f"í•„í„°ë§ í™•ì¸ ì˜¤ë¥˜: {e}")
    
    return (False, None)


def save_articles_to_file(articles, url, selector, filename="scraped_articles.txt"):
    """
    ê²Œì‹œê¸€ ì •ë³´ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        articles: ê²Œì‹œê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        url: ìŠ¤í¬ë˜í•‘ URL
        selector: ì‚¬ìš© CSS Selector
        filename: ì €ì¥í•  íŒŒì¼ëª…
    
    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"ì´ì¹´í˜ ê²Œì‹œê¸€ ì •ë³´ ({len(articles)}ê°œ)\n")
            f.write(f"URL: {url}\n")
            f.write(f"Selector: {selector}\n")
            f.write("="*60 + "\n\n")
            for i, article in enumerate(articles, 1):
                f.write(f"{i}. {article['title']}\n")
                f.write(f"   ID: {article['article_id']}\n")
                f.write(f"   ëŒ“ê¸€ê¸€: {article['comment_count']}ê°œ\n")
                f.write(f"   ì¡°íšŒìˆ˜: {article['read_count']}íšŒ\n")
                f.write(f"   ì¢‹ì•„ìš”: {article['like_count']}ê°œ\n")
                f.write(f"   ìƒì„±ì¼: {article['date']}\n")
                f.write(f"   URL: {article['url']}\n\n")
        Logger.success(f"{filename} ì €ì¥ ì™„ë£Œ")
        return True
    except Exception as e:
        Logger.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
        return False


# ===================== ë©”ì¸ í•¨ìˆ˜ =====================

def scrape_single_page(driver, wait):
    """
    í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œê¸€ì„ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.
    
    Args:
        driver: Selenium WebDriver
        wait: WebDriverWait ê°ì²´
    
    Returns:
        tuple: (articles: list, should_stop: bool)
               - articles: ì¶”ì¶œëœ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸
               - should_stop: ìŠ¤í¬ë˜í•‘ ì¤‘ë‹¨ ì—¬ë¶€
    """
    articles = []
    should_stop = False
    
    try:
        # í˜ì´ì§€ ì™„ì „ ë¡œë”© ëŒ€ê¸°
        Logger.debug("í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° ì¤‘...")
        
        # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ (ìµœëŒ€ 3ë²ˆ)
        table_found = False
        for attempt in range(3):
            try:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table.article-table")))
                Logger.debug(f"í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ! (ì‹œë„ {attempt + 1}/3)")
                table_found = True
                time.sleep(2)  # ì¶”ê°€ ì•ˆì •í™”
                break
            except TimeoutException:
                if attempt < 2:
                    Logger.debug(f"article-table ë¡œë”© ì¬ì‹œë„ ì¤‘... ({attempt + 1}/3)")
                    time.sleep(3)
                else:
                    Logger.warning("article-tableì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if not table_found:
            return articles, True  # ì¤‘ë‹¨
        
        # ê²Œì‹œê¸€ tr) ì°¾ê¸°
        Logger.debug("ê²Œì‹œê¸€ trì„ ì°¾ëŠ” ì¤‘...")
        article_rows = []
        successful_selector = None
        
        for i, selector in enumerate(Config.SELECTORS['article_rows'], 1):
            try:
                Logger.debug(f"[{i}/{len(Config.SELECTORS['article_rows'])}] ?ï¿½ë„ ï¿½? {selector}")
                temp_wait = WebDriverWait(driver, Config.SELECTOR_WAIT)
                temp_wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                article_rows = driver.find_elements(By.CSS_SELECTOR, selector)
                
                if article_rows and len(article_rows) > 0:
                    successful_selector = selector
                    Logger.debug(f"?ï¿½?ï¿½í„° ?ï¿½ê³µ!")
                    break
            except TimeoutException:
                continue
            except Exception as e:
                Logger.debug(f"?ï¿½ë¥˜: {e}")
                continue
        
        if not article_rows:
            Logger.warning("ê²Œì‹œê¸€??ì°¾ï¿½? ëª»í–ˆ?ï¿½ë‹ˆ??")
            return articles, True
        
        Logger.debug(f"ì´ {len(article_rows)}ê°œì˜ í–‰ ë°œê²¬")
        
        # ê° í–‰ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
        skipped_count = 0
        for idx, row in enumerate(article_rows, 1):
            Logger.debug(f"\n--- í–‰ {idx}/{len(article_rows)} ì²˜ë¦¬ ì¤‘ ---")
            
            # í•„í„°ë§ í™•ì¸
            skip, reason = should_skip_article(row, driver, Config.SKIP_NOTICE, Config.SKIP_RECOMMEND)
            if skip:
                Logger.debug(f"í–‰ {idx}: {reason}ìœ¼ë¡œ ìŠ¤í‚µë¨")
                skipped_count += 1
                continue
            
            # ë°ì´í„° ì¶”ì¶œ
            article_data = extract_article_data(row)
            if not article_data:
                Logger.debug(f"í–‰ {idx}: ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨")
                continue
            
            Logger.debug(f"í–‰ {idx}: ì¶”ì¶œ ì„±ê³µ - ì œëª©: {article_data['title'][:30] if article_data['title'] else '(ì œëª©ì—†ìŒ)'}...")
            
            # ë‚ ì§œ í™•ì¸ - ë„ˆë¬´ ì˜¤ë˜ëœ ê²Œì‹œê¸€ì´ë©´ ì¤‘ë‹¨
            if article_data['date']:
                if is_article_too_old(article_data['date'], Config.SCRAPE_DAYS):
                    Logger.info(f"?ï¿½ï¿½ {Config.SCRAPE_DAYS}???ï¿½ì „ ê²Œì‹œê¸€ ë°œê²¬ (?ï¿½ì§œ: {article_data['date']}) - ?ï¿½í¬?ï¿½í•‘ ì¤‘ë‹¨")
                    should_stop = True
                    break
            
            articles.append(article_data)
        
        Logger.debug(f"\n=== í˜ì´ì§€ ì²˜ë¦¬ ê²°ê³¼ ===")
        Logger.debug(f"ì´ í–‰ ìˆ˜: {len(article_rows)}ê°œ")
        Logger.debug(f"ìŠ¤í‚µë¨: {skipped_count}ê°œ")
        Logger.debug(f"ì¶”ì¶œ ì„±ê³µ: {len(articles)}ê°œ")

    except Exception as e:
        Logger.error(f"í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
    
    return articles, should_stop


def scrape_naver_cafe_titles(url):
    """
    ?ï¿½ì´ï¿½?ì¹´í˜ ê²Œì‹œê¸€???ï¿½í¬?ï¿½í•‘?ï¿½ê³  ?ï¿½ì´?ï¿½ë² ?ï¿½ìŠ¤???ï¿½?ï¿½í•©?ï¿½ë‹¤.
    
    Args:
        url: ?ï¿½í¬?ï¿½í•‘???ï¿½ì´ï¿½?ì¹´í˜ URL
    """
    # ë¡œê¹… ?ï¿½ë²¨ ?ï¿½ì •
    Logger.set_level(Config.LOG_LEVEL)
    
    # ?ï¿½ì´?ï¿½ë² ?ï¿½ìŠ¤ ì´ˆê¸°??
    Logger.info("\n?ï¿½ì´?ï¿½ë² ?ï¿½ìŠ¤ ì´ˆê¸°??ï¿½?..")
    db_conn = init_database()
    
    # ?ï¿½ë˜???ï¿½ì´???ï¿½ë¦¬
    cleanup_old_data(db_conn)

    # ë¸Œë¼?ï¿½ï¿½? ?ï¿½ì • ï¿½??ï¿½ì‘
    driver = setup_chrome_driver()
    if not driver:
        db_conn.close()
        return

    # ?ï¿½ì†Œ ?ï¿½ï¿½??ï¿½ì •
    wait = WebDriverWait(driver, Config.ELEMENT_WAIT)
    
    # ?ï¿½ê³„ ë³€??
    total_articles = []
    total_inserted = 0
    total_updated = 0
    total_hot_articles = 0      # ì¸ê¸°ê¸€ ì¹´ìš´í„°
    total_keyword_articles = 0  # í‚¤ì›Œë“œ ì¸ê¸°ê¸€ ì¹´ìš´í„°
    
    try:
        # ?ï¿½ì´ì§€ë³„ë¡œ ?ï¿½í¬?ï¿½í•‘
        for page_num in range(1, Config.MAX_PAGES + 1):
            Logger.separator()
            Logger.info(f"\n?ï¿½ï¿½ ?ï¿½ì´ì§€ {page_num} ?ï¿½í¬?ï¿½í•‘ ï¿½?..")
            Logger.separator()
            
            # ?ï¿½ì´ì§€ URL ?ï¿½ì„± ï¿½??ï¿½ë™
            page_url = generate_page_url(url, page_num)
            Logger.info(f"URL: {page_url}")
            
            try:
                driver.get(page_url)
                Logger.success("URL ë¡œë”© ì‹œì‘...")
                
                # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                Logger.debug(f"í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({Config.PAGE_LOAD_WAIT}ì´ˆ)")
                time.sleep(Config.PAGE_LOAD_WAIT)
                
                # ì‹¤ì œ ë¡œë“œëœ URL í™•ì¸
                current_url = driver.current_url
                Logger.debug(f"í˜„ì¬ URL: {current_url}")
                
            except Exception as e:
                Logger.error(f"URL ë¡œë”© ì‹¤íŒ¨: {e}")
                break
            
            # í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œê¸€ì„ ìŠ¤í¬ë˜í•‘
            page_articles, should_stop = scrape_single_page(driver, wait)
            
            if not page_articles and page_num == 1:
                Logger.error("ì²« í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            if page_articles:
                Logger.info(f"\ní˜ì´ì§€ {page_num}ì˜ ê²Œì‹œê¸€ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥... ({len(page_articles)}ê°œ)")
                inserted = 0
                updated = 0
                hot_count = 0
                keyword_count = 0
                
                for article in page_articles:
                    # ì¼ë°˜ articles í…Œì´ë¸”ì— ì €ì¥
                    result = save_or_update_article(db_conn, article)
                    if result == 'inserted':
                        inserted += 1
                    elif result == 'updated':
                        updated += 1
                    
                    # ì¸ê¸°ê¸€ ì¡°ê±´ ì²´í¬ ë° ì €ì¥
                    if is_hot_article(article):
                        hot_result = save_hot_article(db_conn, article)
                        if hot_result == 'inserted':
                            hot_count += 1
                            total_hot_articles += 1
                        
                        # ì¸ê¸°ê¸€ ì¤‘ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ì²´í¬
                        matched_keywords = check_keywords(article, Config.KEYWORDS)
                        if matched_keywords:
                            keyword_result = save_keyword_article(db_conn, article, matched_keywords)
                            if keyword_result == 'inserted':
                                keyword_count += 1
                                total_keyword_articles += 1
                
                total_inserted += inserted
                total_updated += updated
                total_articles.extend(page_articles)
                
                # ë¡œê·¸ ë©”ì‹œì§€ ìƒì„±
                log_msg = f"í˜ì´ì§€ {page_num} ìŠ¤í¬ë˜í•‘ ì™„ë£Œ - ì‚½ì…: {inserted}ê°œ, ì—…ë°ì´íŠ¸: {updated}ê°œ"
                if hot_count > 0:
                    log_msg += f", ì¸ê¸°ê¸€: {hot_count}ê°œ"
                if keyword_count > 0:
                    log_msg += f", í‚¤ì›Œë“œ ì¸ê¸°ê¸€: {keyword_count}ê°œ"
                Logger.success(log_msg)
            
            # ì¤‘ë‹¨ ì¡°ê±´ ?ï¿½ì¸
            if should_stop:
                Logger.info(f"\ní˜ì´ì§€ {page_num}ì—ì„œ ìŠ¤í¬ë˜í•‘ ì¤‘ë‹¨ ({Config.SCRAPE_DAYS}ì¼ ì´ì „ ê²Œì‹œê¸€ ë°œê²¬)")
                break
            
            # ë§ˆì§€ë§‰ í˜ì´ì§€ì¸ì§€ í™•ì¸
            if len(page_articles) == 0:
                Logger.info(f"\ní˜ì´ì§€ {page_num}ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ")
                break
            
            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if page_num < Config.MAX_PAGES:
                Logger.debug(f"ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™... (3ì´ˆ ëŒ€ê¸°)")
                time.sleep(3)

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        Logger.separator()
        Logger.success(f"\nìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        Logger.separator()
        Logger.info(f"\nìµœì¢… ê²°ê³¼:")
        Logger.info(f"  ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {len(total_articles)}ê°œ")
        Logger.info(f"  ì‚½ì…ëœ ë°ì´í„°: {total_inserted}ê°œ")
        Logger.info(f"  ì—…ë°ì´íŠ¸ëœ ë°ì´í„°: {total_updated}ê°œ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
        stats = get_article_stats(db_conn)
        Logger.info(f"\në°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
        Logger.info(f"  ì´ ê²Œì‹œê¸€: {stats['total_count']}ê°œ")
        Logger.info(f"  ì˜¤ëŠ˜ ì—…ë°ì´íŠ¸ëœ ê²Œì‹œê¸€: {stats['today_updated']}ê°œ")
        if stats['oldest_date']:
            Logger.info(f"  ê°€ì¥ ì˜¤ë˜ëœ ê²Œì‹œê¸€: {stats['oldest_date']}")
        
        # ì¸ê¸°ê¸€ í†µê³„
        hot_stats = get_hot_article_stats(db_conn)
        if total_hot_articles > 0 or hot_stats['total_hot'] > 0:
            Logger.info(f"\nğŸ”¥ ì¸ê¸°ê¸€ í†µê³„:")
            Logger.info(f"  ì´ë²ˆ ì‹¤í–‰ì—ì„œ ë°œê²¬: {total_hot_articles}ê°œ")
            Logger.info(f"  ì´ ì¸ê¸°ê¸€: {hot_stats['total_hot']}ê°œ")
            Logger.info(f"  ì˜¤ëŠ˜ ë°œê²¬ëœ ì¸ê¸°ê¸€: {hot_stats['today_hot']}ê°œ")
            Logger.info(f"  ì•Œë¦¼ ëŒ€ê¸° ì¤‘: {hot_stats['pending_notification']}ê°œ")
            Logger.info(f"\nğŸ’¡ ì¸ê¸°ê¸€ ê¸°ì¤€: ì¢‹ì•„ìš” {Config.HOT_ARTICLE_MIN_LIKE}+ AND ì¡°íšŒìˆ˜ {Config.HOT_ARTICLE_MIN_READ}+ AND ëŒ“ê¸€ {Config.HOT_ARTICLE_MIN_COMMENT}+")
        
        # í‚¤ì›Œë“œ ì¸ê¸°ê¸€ í†µê³„
        keyword_stats = get_keyword_article_stats(db_conn)
        if total_keyword_articles > 0 or keyword_stats['total_keyword'] > 0:
            Logger.info(f"\nâ­ í‚¤ì›Œë“œ ì¸ê¸°ê¸€ í†µê³„:")
            Logger.info(f"  ì´ë²ˆ ì‹¤í–‰ì—ì„œ ë°œê²¬: {total_keyword_articles}ê°œ")
            Logger.info(f"  ì´ í‚¤ì›Œë“œ ì¸ê¸°ê¸€: {keyword_stats['total_keyword']}ê°œ")
            Logger.info(f"  ì˜¤ëŠ˜ ë°œê²¬ëœ í‚¤ì›Œë“œ ì¸ê¸°ê¸€: {keyword_stats['today_keyword']}ê°œ")
            Logger.info(f"  ì•Œë¦¼ ëŒ€ê¸° ì¤‘: {keyword_stats['pending_notification']}ê°œ")
            Logger.info(f"\nğŸ” í‚¤ì›Œë“œ: {', '.join(Config.KEYWORDS)}")
        
        # ìŠ¤í¬ë˜í•‘ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥
        if total_articles:
            Logger.separator()
            save_articles_to_file(total_articles, url, "multi-page", Config.OUTPUT_FILE)
    
    except KeyboardInterrupt:
        Logger.separator()
        Logger.warning("ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
        Logger.separator()

    except Exception as e:
        Logger.separator()
        Logger.error("ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ")
        Logger.separator()
        Logger.info(f"\nì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
        import traceback
        Logger.info("\nì„¸ë¶€ ì˜¤ë¥˜:")
        traceback.print_exc()
    
    finally:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ
        try:
            db_conn.close()
            Logger.debug("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")
        except:
            pass
        
        Logger.separator()
        Logger.info("10ì´ˆ í›„ì— ë¸Œë¼ìš°ì €ê°€ ìë™ìœ¼ë¡œ ì¢…ë£Œë©ë‹ˆë‹¤. (ë°”ë¡œ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+C)")
        try:
            for i in range(10, 0, -1):
                Logger.info(f"\rë‚¨ì€ ì‹œê°„: {i}ì´ˆ..    ")
                time.sleep(1)
            Logger.info("\n")
        except KeyboardInterrupt:
            Logger.info("\n")
        
        Logger.info("ë¸Œë¼ìš°ì €ê°€ ì¢…ë£Œë©ë‹ˆë‹¤...")
        driver.quit()
        Logger.success("ë¸Œë¼ìš°ì €ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        Logger.separator()


if __name__ == "__main__":
    Logger.separator()
    Logger.info("    ë„¤ì´ë²„ ì¹´í˜ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í¼")
    Logger.info(f"    ìµœê·¼ {Config.SCRAPE_DAYS}ì¼ ê°„ì˜ ê²Œì‹œê¸€ ìˆ˜ì§‘")
    Logger.separator()
    scrape_naver_cafe_titles(Config.DEFAULT_URL)
