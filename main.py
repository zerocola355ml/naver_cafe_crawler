import sys
import io

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì • (í•œê¸€ ë° íŠ¹ìˆ˜ë¬¸ì ì¶œë ¥ì„ ìœ„í•´)
# line_buffering=Trueë¡œ ì„¤ì •í•˜ì—¬ ì‹¤ì‹œê°„ ì¶œë ¥ ê°€ëŠ¥
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', line_buffering=True)
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', line_buffering=True)

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException
import os
import time
import re
import sqlite3
from datetime import datetime, timedelta

# ===================== Logger í´ë˜ìŠ¤ =====================
class Logger:
    """ë¡œê¹… ë ˆë²¨ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    VERBOSE = 2  # ëª¨ë“  ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
    INFO = 1     # ì‚¬ìš©ì ì •ë³´ë§Œ ì¶œë ¥
    QUIET = 0    # ìµœì†Œ ì •ë³´ë§Œ ì¶œë ¥ (ì—ëŸ¬/ê²½ê³ )
    
    level = INFO  # ê¸°ë³¸ ë ˆë²¨
    
    @staticmethod
    def set_level(level):
        """ë¡œê¹… ë ˆë²¨ ì„¤ì •"""
        Logger.level = level
    
    @staticmethod
    def debug(msg):
        """ë””ë²„ê·¸ ë©”ì‹œì§€ (VERBOSE ë ˆë²¨ì—ì„œë§Œ ì¶œë ¥)"""
        if Logger.level >= Logger.VERBOSE:
            print(f"[DEBUG] {msg}")
    
    @staticmethod
    def info(msg):
        """ì¼ë°˜ ì •ë³´ ë©”ì‹œì§€ (INFO ë ˆë²¨ ì´ìƒì—ì„œ ì¶œë ¥)"""
        if Logger.level >= Logger.INFO:
            print(msg)
    
    @staticmethod
    def success(msg):
        """ì„±ê³µ ë©”ì‹œì§€ (INFO ë ˆë²¨ ì´ìƒì—ì„œ ì¶œë ¥)"""
        if Logger.level >= Logger.INFO:
            print(f"âœ“ {msg}")
    
    @staticmethod
    def warning(msg):
        """ê²½ê³  ë©”ì‹œì§€ (í•­ìƒ ì¶œë ¥)"""
        print(f"âš ï¸ {msg}")
    
    @staticmethod
    def error(msg):
        """ì—ëŸ¬ ë©”ì‹œì§€ (í•­ìƒ ì¶œë ¥)"""
        print(f"âŒ {msg}")
    
    @staticmethod
    def separator(char="=", length=60):
        """êµ¬ë¶„ì„  (INFO ë ˆë²¨ ì´ìƒì—ì„œ ì¶œë ¥)"""
        if Logger.level >= Logger.INFO:
            print(char * length)

# ===================== ì„¤ì • í´ë˜ìŠ¤ =====================
class Config:
    """ìŠ¤í¬ë˜í•‘ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    # URL ì„¤ì •
    DEFAULT_URL = "https://cafe.naver.com/f-e/cafes/10094499/menus/599?viewType=L&page=1"
    
    # ë¡œê¹… ì„¤ì •
    LOG_LEVEL = Logger.INFO  # VERBOSE(ìƒì„¸), INFO(ë³´í†µ), QUIET(ìµœì†Œ)
    
    # ê²Œì‹œê¸€ í•„í„° ì„¤ì •
    SKIP_NOTICE = True      # ê³µì§€ì‚¬í•­ ì œì™¸
    SKIP_RECOMMEND = True   # ì¶”ì²œê¸€ ì œì™¸
    
    # ë¸Œë¼ìš°ì € ì„¤ì •
    USE_PROFILE = False     # Chrome í”„ë¡œí•„ ì‚¬ìš© ì—¬ë¶€
CHROME_PROFILE_PATH = "C:\\Users\\tlsgj\\AppData\\Local\\Google\\Chrome\\User Data"
PROFILE_DIRECTORY = "Default"

    # íƒ€ì„ì•„ì›ƒ ì„¤ì •
    PAGE_LOAD_WAIT = 10     # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    ELEMENT_WAIT = 20       # ìš”ì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    SELECTOR_WAIT = 3       # ê° ì…€ë ‰í„° ì‹œë„ ì‹œê°„ (ì´ˆ)
    
    # ì¶œë ¥ íŒŒì¼ ì„¤ì •
    OUTPUT_FILE = "scraped_articles.txt"
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    DB_FILE = "naver_cafe_articles.db"  # SQLite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ëª…
    RETENTION_DAYS = 15                  # ë°ì´í„° ë³´ê´€ ê¸°ê°„ (ì¼)
    
    # ìŠ¤í¬ë˜í•‘ ë²”ìœ„ ì„¤ì •
    SCRAPE_DAYS = 7         # ìµœê·¼ ë©°ì¹  ë™ì•ˆì˜ ê²Œì‹œê¸€ë§Œ ìˆ˜ì§‘ (ì˜¤ëŠ˜ë¶€í„° Nì¼ ì „ê¹Œì§€)
    MAX_PAGES = 50          # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
    
    # CSS Selector ì •ì˜
    SELECTORS = {
        # ê²Œì‹œê¸€ í–‰(tr) ì°¾ê¸°
        'article_rows': [
            "table.article-table > tbody:nth-of-type(3) tr",  # ì¼ë°˜ê¸€ë§Œ (3ë²ˆì§¸ tbody)
            "table.article-table tbody:nth-of-type(3) tr",
            "table.article-table tbody:nth-of-type(n+2) tr",  # ì¶”ì²œê¸€ + ì¼ë°˜ê¸€
            "table.article-table tr",  # ì „ì²´
        ],
        
        # ê° í–‰ ë‚´ë¶€ì—ì„œ ì •ë³´ ì¶”ì¶œ
        'article_id': "td.td_normal.type_articleNumber",
        'title': "a.article",
        'comment': "a.cmt",
        'date': "td.td_normal.type_date",
        'read_count': "td.td_normal.type_readCount",
        'like_count': "td.td_normal.type_likeCount",
    }

# ===================== ë°ì´í„°ë² ì´ìŠ¤ í•¨ìˆ˜ =====================

def init_database():
    """
    SQLite ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±í•˜ê³ , ì—°ê²°ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        sqlite3.Connection: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    """
    conn = sqlite3.connect(Config.DB_FILE)
    cursor = conn.cursor()
    
    # í…Œì´ë¸” ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ë¬´ì‹œ)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS articles (
            article_id TEXT PRIMARY KEY,
            title TEXT NOT NULL,
            comment_count INTEGER DEFAULT 0,
            date TEXT,
            read_count INTEGER DEFAULT 0,
            like_count INTEGER DEFAULT 0,
            url TEXT,
            first_scraped TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    conn.commit()
    Logger.debug(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ: {Config.DB_FILE}")
    return conn


def cleanup_old_data(conn):
    """
    ë³´ê´€ ê¸°ê°„ì´ ì§€ë‚œ ì˜¤ë˜ëœ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        conn: SQLite ì—°ê²°
    
    Returns:
        int: ì‚­ì œëœ í–‰ì˜ ê°œìˆ˜
    """
    cursor = conn.cursor()
    cutoff_date = datetime.now() - timedelta(days=Config.RETENTION_DAYS)
    
    cursor.execute('''
        DELETE FROM articles 
        WHERE last_updated < ?
    ''', (cutoff_date.strftime('%Y-%m-%d %H:%M:%S'),))
    
    deleted_count = cursor.rowcount
    conn.commit()
    
    if deleted_count > 0:
        Logger.info(f"{deleted_count}ê°œì˜ ì˜¤ë˜ëœ ê²Œì‹œê¸€ ì‚­ì œë¨ ({Config.RETENTION_DAYS}ì¼ ì´ìƒ)")
    
    return deleted_count


def save_or_update_article(conn, article):
    """
    ê²Œì‹œê¸€ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    
    Args:
        conn: SQLite ì—°ê²°
        article: ê²Œì‹œê¸€ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        str: 'inserted', 'updated', ë˜ëŠ” 'error'
    """
    cursor = conn.cursor()
    
    try:
        # ê¸°ì¡´ ê²Œì‹œê¸€ì´ ìˆëŠ”ì§€ í™•ì¸
        cursor.execute('SELECT article_id FROM articles WHERE article_id = ?', (article['article_id'],))
        exists = cursor.fetchone()
        
        now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        if exists:
            # ì—…ë°ì´íŠ¸
            cursor.execute('''
                UPDATE articles 
                SET title = ?, comment_count = ?, date = ?, 
                    read_count = ?, like_count = ?, url = ?, last_updated = ?
                WHERE article_id = ?
            ''', (
                article['title'], article['comment_count'], article['date'],
                article['read_count'], article['like_count'], article['url'], 
                now, article['article_id']
            ))
            conn.commit()
            return 'updated'
        else:
            # ì‚½ì…
            cursor.execute('''
                INSERT INTO articles 
                (article_id, title, comment_count, date, read_count, like_count, url, first_scraped, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article['article_id'], article['title'], article['comment_count'],
                article['date'], article['read_count'], article['like_count'],
                article['url'], now, now
            ))
            conn.commit()
            return 'inserted'
    except Exception as e:
        Logger.debug(f"DB ì €ì¥ ì˜¤ë¥˜ (ID: {article.get('article_id')}): {e}")
        return 'error'


def get_article_stats(conn):
    """
    ë°ì´í„°ë² ì´ìŠ¤ì˜ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        conn: SQLite ì—°ê²°
    
    Returns:
        dict: í†µê³„ ì •ë³´
    """
    cursor = conn.cursor()
    
    # ì´ ê²Œì‹œê¸€ ìˆ˜
    cursor.execute('SELECT COUNT(*) FROM articles')
    total_count = cursor.fetchone()[0]
    
    # ì˜¤ëŠ˜ ì—…ë°ì´íŠ¸ëœ ê²Œì‹œê¸€ ìˆ˜
    today = datetime.now().strftime('%Y-%m-%d')
    cursor.execute('SELECT COUNT(*) FROM articles WHERE DATE(last_updated) = ?', (today,))
    today_updated = cursor.fetchone()[0]
    
    # ê°€ì¥ ì˜¤ë˜ëœ ê²Œì‹œê¸€
    cursor.execute('SELECT MIN(first_scraped) FROM articles')
    oldest = cursor.fetchone()[0]
    
    return {
        'total_count': total_count,
        'today_updated': today_updated,
        'oldest_date': oldest
    }


# ===================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =====================

def parse_article_date(date_str):
    """
    ê²Œì‹œê¸€ ì‘ì„±ì¼ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    
    Args:
        date_str: ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: "09:05" ë˜ëŠ” "2025.10.26.")
    
    Returns:
        datetime: íŒŒì‹±ëœ ë‚ ì§œ ë˜ëŠ” None
    """
    try:
        date_str = date_str.strip()
        
        # ì‹œê°„ í˜•íƒœ (ì˜¤ëŠ˜ ê²Œì‹œê¸€): "09:05"
        if ':' in date_str and '.' not in date_str:
            today = datetime.now()
            time_parts = date_str.split(':')
            hour = int(time_parts[0])
            minute = int(time_parts[1])
            return today.replace(hour=hour, minute=minute, second=0, microsecond=0)
        
        # ë‚ ì§œ í˜•íƒœ: "2025.10.26." ë˜ëŠ” "2025.10.26"
        elif '.' in date_str:
            date_str = date_str.rstrip('.')
            parts = date_str.split('.')
            if len(parts) >= 3:
                year = int(parts[0])
                month = int(parts[1])
                day = int(parts[2])
                return datetime(year, month, day)
        
        return None
    except Exception as e:
        Logger.debug(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_str} - {e}")
        return None


def is_article_too_old(date_str, days_limit):
    """
    ê²Œì‹œê¸€ì´ ë„ˆë¬´ ì˜¤ë˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        date_str: ë‚ ì§œ ë¬¸ìì—´
        days_limit: ë©°ì¹  ì „ê¹Œì§€ ìˆ˜ì§‘í• ì§€
    
    Returns:
        bool: Trueë©´ ë„ˆë¬´ ì˜¤ë˜ë¨ (ì¤‘ë‹¨í•´ì•¼ í•¨)
    """
    article_date = parse_article_date(date_str)
    if not article_date:
        return False
    
    cutoff_date = datetime.now() - timedelta(days=days_limit)
    return article_date < cutoff_date


def setup_chrome_driver():
    """
    Chrome WebDriverë¥¼ ì„¤ì •í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        webdriver.Chrome: ì„¤ì •ëœ Chrome WebDriver
    """
    options = Options()

    # í”„ë¡œí•„ ì‚¬ìš© ì„¤ì •
    if Config.USE_PROFILE:
        Logger.debug("í”„ë¡œí•„ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        profile_path = os.path.join(Config.CHROME_PROFILE_PATH, Config.PROFILE_DIRECTORY)
        if not os.path.isdir(profile_path):
            Logger.error("í”„ë¡œí•„ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. USE_PROFILEì„ Falseë¡œ ì„¤ì •í•˜ì„¸ìš”.")
            return None
        options.add_argument(f"user-data-dir={Config.CHROME_PROFILE_PATH}")
        options.add_argument(f"profile-directory={Config.PROFILE_DIRECTORY}")
    
    # Chrome ì˜µì…˜ (ë´‡ ê°ì§€ ìš°íšŒ)
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument("--start-maximized")
    
    try:
        Logger.info("\në¸Œë¼ìš°ì €ë¥¼ ì‹œì‘í•˜ëŠ” ì¤‘...")
        driver = webdriver.Chrome(options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        Logger.success("ë¸Œë¼ìš°ì €ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return driver
    except Exception as e:
        Logger.error("WebDriver ì‹¤í–‰ ì‹¤íŒ¨.")
        Logger.info(f"ì˜¤ë¥˜: {e}")
        Logger.info("\ní•´ê²° ë°©ë²•:")
        Logger.info("1. Chrome ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
        Logger.info("2. ChromeDriverê°€ ìë™ìœ¼ë¡œ ì„¤ì¹˜ë˜ëŠ”ì§€ í™•ì¸")
        return None


def extract_article_data(row):
    """
    í–‰(tr)ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        row: Selenium WebElement (tr íƒœê·¸)
    
    Returns:
        dict: ê²Œì‹œê¸€ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None (ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ)
    """
    article_data = {
        'article_id': '',
        'title': '',
        'comment_count': 0,
        'date': '',
        'read_count': 0,
        'like_count': 0,
        'url': ''
    }
    
    try:
        # ê²Œì‹œê¸€ ID ì¶”ì¶œ (tdì—ì„œ ì§ì ‘)
        try:
            article_id_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['article_id'])
            article_data['article_id'] = article_id_elem.text.strip()
        except:
            pass
        
        # ì œëª© ì¶”ì¶œ (í•„ìˆ˜)
        try:
            title_link = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['title'])
            article_data['title'] = title_link.text.strip()
            article_data['url'] = title_link.get_attribute('href')
            
            # IDê°€ ì—†ìœ¼ë©´ URLì—ì„œ ì¶”ì¶œ
            if not article_data['article_id']:
                url = article_data['url']
                if 'articleid=' in url.lower():
                    article_data['article_id'] = url.split('articleid=')[1].split('&')[0]
                elif '/articles/' in url:
                    article_data['article_id'] = url.split('/articles/')[1].split('?')[0]
        except:
            return None
        
        # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
        try:
            comment_link = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['comment'])
            comment_text = comment_link.text.strip()
            numbers = re.findall(r'\d+', comment_text)
            if numbers:
                article_data['comment_count'] = int(numbers[0])
        except:
            article_data['comment_count'] = 0
        
        # ì‘ì„±ì¼ ì¶”ì¶œ
        try:
            date_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['date'])
            article_data['date'] = date_elem.text.strip()
        except:
            article_data['date'] = ''
        
        # ì¡°íšŒìˆ˜ ì¶”ì¶œ
        try:
            read_count_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['read_count'])
            read_count_text = read_count_elem.text.strip()
            numbers = re.findall(r'\d+', read_count_text.replace(',', ''))
            if numbers:
                article_data['read_count'] = int(numbers[0])
        except:
            article_data['read_count'] = 0
        
        # ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ
        try:
            like_count_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['like_count'])
            like_count_text = like_count_elem.text.strip()
            numbers = re.findall(r'\d+', like_count_text.replace(',', ''))
            if numbers:
                article_data['like_count'] = int(numbers[0])
        except:
            article_data['like_count'] = 0
        
        return article_data
        
    except Exception as e:
        Logger.debug(f"ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None


def should_skip_article(row, driver, skip_notice=True, skip_recommend=True):
    """
    ê²Œì‹œê¸€ì´ ê³µì§€ì‚¬í•­ ë˜ëŠ” ì¶”ì²œê¸€ì¸ì§€ í™•ì¸í•˜ì—¬ í•„í„°ë§ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    
    Args:
        row: Selenium WebElement (tr íƒœê·¸)
        driver: Selenium WebDriver
        skip_notice: ê³µì§€ì‚¬í•­ ì œì™¸ ì—¬ë¶€
        skip_recommend: ì¶”ì²œê¸€ ì œì™¸ ì—¬ë¶€
    
    Returns:
        tuple: (skip: bool, reason: str) - ê±´ë„ˆë›¸ì§€ ì—¬ë¶€ì™€ ì´ìœ 
    """
    try:
        # í–‰ì´ ì†í•œ tbody ì°¾ê¸°
        tbody = driver.execute_script("""
            let elem = arguments[0];
            while (elem && elem.tagName !== 'TBODY') {
                elem = elem.parentElement;
            }
            return elem;
        """, row)
        
        if tbody:
            # tbodyì˜ ë¶€ëª¨ table ì°¾ê¸°
            table = driver.execute_script("return arguments[0].parentElement;", tbody)
            if table and 'article-table' in (table.get_attribute('class') or ''):
                # ê°™ì€ tableì˜ ëª¨ë“  tbody ê°€ì ¸ì˜¤ê¸°
                all_tbodies = driver.execute_script(
                    "return arguments[0].querySelectorAll('tbody');", table
                )
                # í˜„ì¬ tbodyì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                tbody_index = -1
                for idx, tb in enumerate(all_tbodies):
                    if tb == tbody:
                        tbody_index = idx
                        break
                
                # 0: ê³µì§€ì‚¬í•­, 1: ì¶”ì²œê¸€, 2: ì¼ë°˜ê¸€
                if tbody_index == 0 and skip_notice:
                    return (True, 'notice')
                elif tbody_index == 1 and skip_recommend:
                    return (True, 'recommend')
    except Exception as e:
        Logger.debug(f"í•„í„°ë§ í™•ì¸ ì˜¤ë¥˜: {e}")
    
    return (False, None)


def save_articles_to_file(articles, url, selector, filename="scraped_articles.txt"):
    """
    ê²Œì‹œê¸€ ì •ë³´ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        articles: ê²Œì‹œê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        url: ìŠ¤í¬ë˜í•‘í•œ URL
        selector: ì‚¬ìš©ëœ CSS Selector
        filename: ì €ì¥í•  íŒŒì¼ëª…
    
    Returns:
        bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"ë„¤ì´ë²„ ì¹´í˜ ê²Œì‹œê¸€ ì •ë³´ ({len(articles)}ê°œ)\n")
            f.write(f"URL: {url}\n")
            f.write(f"Selector: {selector}\n")
            f.write("="*60 + "\n\n")
            for i, article in enumerate(articles, 1):
                f.write(f"{i}. {article['title']}\n")
                f.write(f"   ID: {article['article_id']}\n")
                f.write(f"   ëŒ“ê¸€: {article['comment_count']}ê°œ\n")
                f.write(f"   ì¡°íšŒìˆ˜: {article['read_count']}\n")
                f.write(f"   ì¢‹ì•„ìš”: {article['like_count']}\n")
                f.write(f"   ì‘ì„±ì¼: {article['date']}\n")
                f.write(f"   URL: {article['url']}\n\n")
        Logger.success(f"{filename} íŒŒì¼ë¡œë„ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        Logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


# ===================== ë©”ì¸ í•¨ìˆ˜ =====================

def scrape_naver_cafe_titles(url):
    """
    ë„¤ì´ë²„ ì¹´í˜ ê²Œì‹œê¸€ì„ ìŠ¤í¬ë˜í•‘í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        url: ìŠ¤í¬ë˜í•‘í•  ë„¤ì´ë²„ ì¹´í˜ URL
    """
    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    Logger.set_level(Config.LOG_LEVEL)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    Logger.info("\në°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    db_conn = init_database()
    
    # ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
    cleanup_old_data(db_conn)

    # ë¸Œë¼ìš°ì € ì„¤ì • ë° ì‹œì‘
    driver = setup_chrome_driver()
    if not driver:
        db_conn.close()
        return

    # 2. URLë¡œ ì´ë™
    try:
        Logger.info(f"\nëŒ€ìƒ URLë¡œ ì´ë™ ì¤‘: {url}")
        driver.get(url)
        Logger.success("URL ë¡œë”© ì‹œì‘...")
        
        # í˜ì´ì§€ ì´ˆê¸° ë¡œë”© ëŒ€ê¸°
        Logger.info(f"í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘... ({Config.PAGE_LOAD_WAIT}ì´ˆ)")
        time.sleep(Config.PAGE_LOAD_WAIT)
        Logger.success("ë¡œë”© ì™„ë£Œ, ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")

    except Exception as e:
        Logger.error(f"URL ì´ë™ ì‹¤íŒ¨: {e}")
        driver.quit()
        return

    # 3. ìš”ì†Œ ëŒ€ê¸° ì„¤ì •
    wait = WebDriverWait(driver, Config.ELEMENT_WAIT)

    try:
        # 3. í˜ì´ì§€ ì™„ì „ ë¡œë”© ëŒ€ê¸° (ìƒˆë¡œìš´ ë„¤ì´ë²„ ì¹´í˜ UIëŠ” iframe ì—†ìŒ)
        Logger.info("\ní˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° ì¤‘...")
        try:
            # article-tableì´ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table.article-table")))
            Logger.success("í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ!")
            time.sleep(2)  # ì¶”ê°€ ì•ˆì •í™” ëŒ€ê¸°
        except TimeoutException:
            Logger.warning("article-tableì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³„ì† ì‹œë„í•©ë‹ˆë‹¤...")
            time.sleep(3)

        # 4. ê²Œì‹œê¸€ í–‰(tr) ì°¾ê¸°
        Logger.info("\nê²Œì‹œê¸€ í–‰ì„ ì°¾ëŠ” ì¤‘...")
        article_rows = []
        successful_selector = None
        
        for i, selector in enumerate(Config.SELECTORS['article_rows'], 1):
            try:
                Logger.debug(f"[{i}/{len(Config.SELECTORS['article_rows'])}] ì‹œë„ ì¤‘: {selector}")
                temp_wait = WebDriverWait(driver, Config.SELECTOR_WAIT)
                temp_wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                article_rows = driver.find_elements(By.CSS_SELECTOR, selector)
                
                if article_rows and len(article_rows) > 0:
                    successful_selector = selector
                    Logger.success(f"ì…€ë ‰í„° [{i}/{len(Config.SELECTORS['article_rows'])}] ì„±ê³µ!")
                    break
                else:
                    Logger.debug("(ìš”ì†Œ ì—†ìŒ)")
            except TimeoutException:
                Logger.debug("(íƒ€ì„ì•„ì›ƒ)")
                continue
            except Exception as e:
                Logger.debug(f"(ì˜¤ë¥˜: {e})")
                continue

        # 5. ê° í–‰ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
        if not article_rows or len(article_rows) == 0:
            Logger.separator()
            Logger.error("ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            Logger.separator()
            Logger.info(f"\ní˜„ì¬ URL: {driver.current_url}")
            
            # ë””ë²„ê¹…ìš© íŒŒì¼ ì €ì¥
            Logger.info("\në””ë²„ê¹…ìš© íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤...")
            try:
                with open("debug_page_source.html", "w", encoding="utf-8") as f:
                    f.write(driver.page_source)
                Logger.success("debug_page_source.html ì €ì¥ ì™„ë£Œ")
                
                driver.save_screenshot("debug_screenshot.png")
                Logger.success("debug_screenshot.png ì €ì¥ ì™„ë£Œ")
                
                # í˜ì´ì§€ì—ì„œ 'article' í¬í•¨ ìš”ì†Œ ì°¾ê¸°
                Logger.info("\nğŸ’¡ í˜ì´ì§€ ë¶„ì„ ì¤‘...")
                all_links = driver.find_elements(By.TAG_NAME, "a")
                article_links = [link for link in all_links if 'article' in link.get_attribute('href').lower() if link.get_attribute('href')]
                
                Logger.info(f"  - ì „ì²´ ë§í¬ ê°œìˆ˜: {len(all_links)}")
                Logger.info(f"  - 'article' í¬í•¨ ë§í¬: {len(article_links)}")
                
                if article_links:
                    Logger.info("\n  ìƒ˜í”Œ ë§í¬ (ì²˜ìŒ 3ê°œ):")
                    for i, link in enumerate(article_links[:3], 1):
                        Logger.info(f"    {i}. href: {link.get_attribute('href')}")
                        Logger.info(f"       class: {link.get_attribute('class')}")
                        Logger.info(f"       text: {link.text[:50] if link.text else '(ì—†ìŒ)'}")
                        
                Logger.info("\nğŸ’¡ debug_page_source.html íŒŒì¼ì„ ì—´ì–´ ì •í™•í•œ ì…€ë ‰í„°ë¥¼ ì°¾ì•„ë³´ì„¸ìš”.")
            except Exception as e:
                Logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
                
        else:
            Logger.separator()
            Logger.success(f"ì„±ê³µ! ì´ {len(article_rows)}ê°œì˜ í–‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
            Logger.separator()
            
            # VERBOSE ëª¨ë“œ: ì²˜ìŒ 3ê°œ í–‰ì˜ HTML êµ¬ì¡° ì¶œë ¥
            if Logger.level >= Logger.VERBOSE:
                print("\nğŸ” ë””ë²„ê·¸: ì²˜ìŒ 3ê°œ í–‰ì˜ HTML êµ¬ì¡° ë¶„ì„")
                print("-" * 60)
                for i, row in enumerate(article_rows[:3], 1):
                    try:
                        print(f"\n[í–‰ {i}]")
                        
                        # ê²Œì‹œê¸€ ID
                        try:
                            article_id_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['article_id'])
                            print(f"  ê²Œì‹œê¸€ ID: {article_id_elem.text.strip()}")
                        except:
                            print(f"  ê²Œì‹œê¸€ ID: ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        
                        # ì œëª© ì°¾ê¸°
                        try:
                            title_link = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['title'])
                            print(f"  ì œëª©: {title_link.text.strip()}")
                        except:
                            print(f"  ì œëª©: ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        
                        # ëŒ“ê¸€ ìˆ˜
                        try:
                            comment_link = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['comment'])
                            print(f"  ëŒ“ê¸€ ìˆ˜: {comment_link.text.strip()}")
                        except:
                            print(f"  ëŒ“ê¸€ ìˆ˜: 0")
                        
                        # ì‘ì„±ì¼
                        try:
                            date_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['date'])
                            print(f"  ì‘ì„±ì¼: {date_elem.text.strip()}")
                        except:
                            print(f"  ì‘ì„±ì¼: ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        
                        # ì¡°íšŒìˆ˜
                        try:
                            read_count_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['read_count'])
                            print(f"  ì¡°íšŒìˆ˜: {read_count_elem.text.strip()}")
                        except:
                            print(f"  ì¡°íšŒìˆ˜: 0")
                        
                        # ì¢‹ì•„ìš” ìˆ˜
                        try:
                            like_count_elem = row.find_element(By.CSS_SELECTOR, Config.SELECTORS['like_count'])
                            print(f"  ì¢‹ì•„ìš” ìˆ˜: {like_count_elem.text.strip()}")
                        except:
                            print(f"  ì¢‹ì•„ìš” ìˆ˜: 0")
                        
                        # tbody ì¸ë±ìŠ¤ í™•ì¸
                        tbody = driver.execute_script("""
                            let elem = arguments[0];
                            while (elem && elem.tagName !== 'TBODY') {
                                elem = elem.parentElement;
                            }
                            return elem;
                        """, row)
                        
                        if tbody:
                            table = driver.execute_script("return arguments[0].parentElement;", tbody)
                            if table:
                                all_tbodies = driver.execute_script(
                                    "return arguments[0].querySelectorAll('tbody');", table
                                )
                                tbody_index = -1
                                for idx, tb in enumerate(all_tbodies):
                                    if tb == tbody:
                                        tbody_index = idx
                                        break
                                
                                tbody_type = "ì•Œ ìˆ˜ ì—†ìŒ"
                                if tbody_index == 0:
                                    tbody_type = "ê³µì§€ì‚¬í•­"
                                elif tbody_index == 1:
                                    tbody_type = "ì¶”ì²œê¸€"
                                elif tbody_index == 2:
                                    tbody_type = "ì¼ë°˜ê¸€"
                                
                                print(f"  ğŸ¯ tbody ì¸ë±ìŠ¤: {tbody_index} ({tbody_type})")
                        
                        # í–‰(tr) íƒœê·¸ ì •ë³´
                        print(f"  í–‰ íƒœê·¸: {row.tag_name}")
                        print(f"  í–‰ class: {row.get_attribute('class')}")
                    except Exception as e:
                        print(f"  ë¶„ì„ ì˜¤ë¥˜: {e}")
                print("-" * 60)
                
                print("\nâ¸ ë¶„ì„ ê²°ê³¼ í™•ì¸ì„ ìœ„í•´ 15ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤... (ìŠ¤í‚µí•˜ë ¤ë©´ Ctrl+C)")
                try:
                    for i in range(15, 0, -1):
                        print(f"\rë‚¨ì€ ì‹œê°„: {i}ì´ˆ...    ", end="")
                        time.sleep(1)
                    print("\nâœ“ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                except KeyboardInterrupt:
                    print("\nâœ“ ëŒ€ê¸° ìŠ¤í‚µ, ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
            
            # ê° í–‰ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ
            articles = []
            skipped_notice = 0
            skipped_recommend = 0
            
            Logger.info("\nê²Œì‹œê¸€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘...")
            for row in article_rows:
                # í•„í„°ë§ í™•ì¸
                skip, reason = should_skip_article(row, driver, Config.SKIP_NOTICE, Config.SKIP_RECOMMEND)
                if skip:
                    if reason == 'notice':
                        skipped_notice += 1
                    elif reason == 'recommend':
                        skipped_recommend += 1
                    continue
                
                # ë°ì´í„° ì¶”ì¶œ
                article_data = extract_article_data(row)
                if article_data:
                    articles.append(article_data)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥/ì—…ë°ì´íŠ¸
            Logger.info(f"\në°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
            inserted_count = 0
            updated_count = 0
            error_count = 0
            
            for article in articles:
                result = save_or_update_article(db_conn, article)
                if result == 'inserted':
                    inserted_count += 1
                elif result == 'updated':
                    updated_count += 1
                elif result == 'error':
                    error_count += 1
            
            Logger.success(f"DB ì €ì¥ ì™„ë£Œ - ìƒˆ ê²Œì‹œê¸€: {inserted_count}ê°œ, ì—…ë°ì´íŠ¸: {updated_count}ê°œ")
            if error_count > 0:
                Logger.warning(f"ì €ì¥ ì‹¤íŒ¨: {error_count}ê°œ")
            
            # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
            stats = get_article_stats(db_conn)
            Logger.info(f"\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
            Logger.info(f"  ì´ ê²Œì‹œê¸€: {stats['total_count']}ê°œ")
            Logger.info(f"  ì˜¤ëŠ˜ ì—…ë°ì´íŠ¸: {stats['today_updated']}ê°œ")
            if stats['oldest_date']:
                Logger.info(f"  ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°: {stats['oldest_date']}")
            
            Logger.separator()
            Logger.info(f"ì‚¬ìš©ëœ CSS Selector: {successful_selector}")
            Logger.info(f"ì´ë²ˆ ìŠ¤í¬ë˜í•‘ ë°œê²¬: {len(article_rows)}ê°œ")
            if Config.SKIP_NOTICE:
                Logger.info(f"ê³µì§€ì‚¬í•­ ì œì™¸: {skipped_notice}ê°œ")
            if Config.SKIP_RECOMMEND:
                Logger.info(f"ì¶”ì²œê¸€ ì œì™¸: {skipped_recommend}ê°œ")
            Logger.info(f"ìµœì¢… ê²Œì‹œê¸€ ê°œìˆ˜: {len(articles)}ê°œ\n")
            
            for i, article in enumerate(articles, 1):
                Logger.info(f"{i:3d}. {article['title']}")
                Logger.info(f"      ID: {article['article_id']} | ëŒ“ê¸€: {article['comment_count']} | ì¡°íšŒ: {article['read_count']} | ì¢‹ì•„ìš”: {article['like_count']} | ë‚ ì§œ: {article['date']}")
            
            Logger.separator()
            
            # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œë„ ì €ì¥
            save_articles_to_file(articles, url, successful_selector, Config.OUTPUT_FILE)

    except TimeoutException:
        Logger.separator()
        Logger.error("í˜ì´ì§€ ìš”ì†Œ ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
        Logger.separator()
        Logger.info("\nâ–¶ ê°€ëŠ¥í•œ ì›ì¸:")
        Logger.info("  1. ë¡œê·¸ì¸ì´ í•„ìš”í•œë° ë¡œê·¸ì¸í•˜ì§€ ì•ŠìŒ")
        Logger.info("  2. ë„¤ì´ë²„ ì¹´í˜ì— ê°€ì…í•˜ì§€ ì•Šì•˜ê±°ë‚˜ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŒ")
        Logger.info("  3. ì¹´í˜ êµ¬ì¡°ê°€ ë³€ê²½ë¨")
        Logger.info(f"\ní˜„ì¬ URL: {driver.current_url}")
        
        # ë””ë²„ê¹…ìš© ìŠ¤í¬ë¦°ìƒ·
        try:
            driver.save_screenshot("debug_screenshot.png")
            Logger.success("ë””ë²„ê¹…ìš© ìŠ¤í¬ë¦°ìƒ·ì´ debug_screenshot.pngë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except:
            pass
            
    except Exception as e:
        Logger.separator()
        Logger.error("ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ")
        Logger.separator()
        Logger.info(f"\nì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
        import traceback
        Logger.info("\nìƒì„¸ ì˜¤ë¥˜:")
        traceback.print_exc()
    finally:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ
        try:
            db_conn.close()
            Logger.debug("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")
        except:
            pass
        
        Logger.separator()
        Logger.info("10ì´ˆ í›„ ë¸Œë¼ìš°ì €ê°€ ìë™ìœ¼ë¡œ ì¢…ë£Œë©ë‹ˆë‹¤... (ë°”ë¡œ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+C)")
        try:
            for i in range(10, 0, -1):
                Logger.info(f"\rë‚¨ì€ ì‹œê°„: {i}ì´ˆ...    ")
                time.sleep(1)
            Logger.info("\n")
        except KeyboardInterrupt:
            Logger.info("\n")
        
        Logger.info("ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•˜ëŠ” ì¤‘...")
        driver.quit()
        Logger.success("ë¸Œë¼ìš°ì €ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        Logger.separator()


if __name__ == "__main__":
    Logger.separator()
    Logger.info("    ë„¤ì´ë²„ ì¹´í˜ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í¼")
    Logger.separator()
    scrape_naver_cafe_titles(Config.DEFAULT_URL)
